---
---

@inproceedings{saha2021position,
  abbr={DI@KDD},
  title={Position Masking for Improved Layout-Aware Document Understanding},
  author={Saha, Anik and Finegan-Dollak, Catherine and Verma, Ashish},
  journal={arXiv preprint arXiv:2109.00442},
  booktitle={Document Intelligence Workshop at KDD},
  arxiv={2109.00442},
  pdf={https://document-intelligence.github.io/DI-2021/files/di-2021_final_21.pdf},
  slides={https://document-intelligence.github.io/DI-2021/files/di-2021_slides_21.pptx},
  video={https://youtu.be/2XqrIHexte0},
  file={saha2021position.pdf},
  year={2021},
  abstract={Natural language processing for document scans and PDFs has the potential to enormously improve the efficiency of business processes. Layout-aware word embeddings such as LayoutLM have shown promise for classification of and information extraction from such documents. This paper proposes a new pre-training task called position masking that can improve performance of layout-aware word embeddings that incorporate 2-D position embeddings. We compare models pre-trained with only language masking against models pre-trained with both language masking and position masking, and we find that position masking improves performance by over 5% on a form understanding task.}
}


@inproceedings{saha2022spock2,
  abbr={CASE2022},
  title={SPOCK @ Causal News Corpus 2022: Cause-Effect-Signal Span Detection Using Span-Based and Sequence Tagging Models},
  author={Saha, Anik and Gittens, Alex and Ni, Jian and Hassanzadeh, Oktie and Yener, Bulent and Srinivas, Kavitha},
  booktitle={Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)},
  pages={133--137},
  file={saha2022spock2.pdf},
  year={2022},
  abstract={Understanding causal relationship is an importance part of natural language processing. We address the causal information extraction problem with different neural models built on top of pre-trained transformer-based language models for identifying Cause, Effect and Signal spans, from news data sets. We use the Causal News Corpus subtask 2 training data set to train span-based and sequence tagging models. Our span-based model based on pre-trained BERT base weights achieves an F1 score of 47.48 on the test set with an accuracy score of 36.87 and obtained 3rd place in the Causal News Corpus 2022 shared task.}
}

@inproceedings{saha2022spock,
  abbr={FinCausal2022},
  title={SPOCK at FinCausal 2022: Causal Information Extraction Using Span-Based and Sequence Tagging Models},
  author={Saha, Anik and Ni, Jian and Hassanzadeh, Oktie and Gittens, Alex and Srinivas, Kavitha and Yener, Bulent},
  booktitle={Proceedings of the 4th Financial Narrative Processing Workshop@ LREC2022},
  pages={108--111},
  file={saha2022spock.pdf},
  year={2022},
  abstract={Causal information extraction is an important task in natural language processing, particularly in finance domain. In this work, we develop several information extraction models using pre-trained transformer-based language models for identifying cause and effect text spans from financial documents. We use FinCausal 2021 and 2022 data sets to train span-based and sequence tagging models. Our ensemble of sequence tagging models based on the RoBERTa-Large pre-trained language model achieves an F1 score of 94.70 with Exact Match score of 85.85 and obtains the 1st place in the FinCausal 2022 competition.}
}


