<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Publications - Anik Saha</title>
  <meta name="description" content="Homepage of Anik Saha">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="/publications/">
  <link rel="shortcut icon" type ="image/x-icon" href="/favicon.ico">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <link rel="preconnect" href="https://player.vimeo.com">
  <link rel="preconnect" href="https://i.vimeocdn.com">
  <link rel="preconnect" href="https://f.vimeocdn.com">



<!-- Google Analytics (original) -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

</script>

<!-- Global site tag (gtag.js) - Google Analytics 4 -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '');
</script>

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
<!-- End Google Tag Manager -->



</head>


  <body>

    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<nav class="navbar sticky-top navbar-expand-md navbar-dark bg-dark">
    <a class="navbar-brand" href="/">
     <img src="/favicon.ico" width="30" height="30" style="margin-right:5px" class="d-inline-block align-top" alt="">
      Anik Saha
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarColor02">
        <ul class="navbar-nav mr-auto">
        <ul class="navbar-nav">
          <li class="nav-item">
              <a class="nav-link" href="/">Home</a>
          </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/about">About</a>
           </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/publications">Publications</a>
           </li> 
          
          <li class="nav-item">
            <a class="nav-link"  href="/assets/Resume_Anik.pdf" target="_blank">Resume</a>
          </li>        
        </ul>
  </div>
</nav>



    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12 col-xs-12">
  <style>
.jumbotron{
    padding:3%;
    padding-bottom:10px;
    padding-top:10px;
    margin-top:10px;
    margin-bottom:30px;
}
</style>

<!-- <div class="jumbotron">
### Refereed journal articles
<ol class="bibliography" reversed="reversed"></ol>
</div> -->

<div class="jumbotron">
  <!-- ### Conference proceedings -->
  <ol class="bibliography" reversed="reversed"><li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="saha2022spock2">Saha, A., Gittens, A., Ni, J., Hassanzadeh, O., Yener, B., &amp; Srinivas, K. (2022). SPOCK @ Causal News Corpus 2022: Cause-Effect-Signal Span Detection Using Span-Based and Sequence Tagging Models. <i>Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-Political Events from Text (CASE)</i>, 133–137.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="saha2022spock2">Saha, A., Gittens, A., Ni, J., Hassanzadeh, O., Yener, B., &amp; Srinivas, K. (2022). SPOCK @ Causal News Corpus 2022: Cause-Effect-Signal Span Detection Using Span-Based and Sequence Tagging Models. <i>Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-Political Events from Text (CASE)</i>, 133–137.</span></div> -->
<!-- <span id="saha2022spock2"><b>Saha, A.</b>, Gittens, A., Ni, J., Hassanzadeh, O., Yener, B., &amp; Srinivas, K. (2022). SPOCK @ Causal News Corpus 2022: Cause-Effect-Signal Span Detection Using Span-Based and Sequence Tagging Models. <i>Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-Political Events from Text (CASE)</i>, 133–137.</span></div> -->





<a href="/papers/saha2022spock2.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>





<button class="btn btn-danger btm-sm" onclick="toggleBibtexsaha2022spock2()">BIB</button>



<button class="btn btn-warning btm-sm" onclick="toggleAbstractsaha2022spock2()">ABSTRACT</button>



<div id="asaha2022spock2" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{saha2022spock2,
  abbr = {CASE2022},
  title = {SPOCK @ Causal News Corpus 2022: Cause-Effect-Signal Span Detection Using Span-Based and Sequence Tagging Models},
  author = {Saha, Anik and Gittens, Alex and Ni, Jian and Hassanzadeh, Oktie and Yener, Bulent and Srinivas, Kavitha},
  booktitle = {Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)},
  pages = {133--137},
  file = {saha2022spock2.pdf},
  year = {2022}
}
</pre>
</div>


<div id="bsaha2022spock2" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Understanding causal relationship is an importance part of natural language processing. We address the causal information extraction problem with different neural models built on top of pre-trained transformer-based language models for identifying Cause, Effect and Signal spans, from news data sets. We use the Causal News Corpus subtask 2 training data set to train span-based and sequence tagging models. Our span-based model based on pre-trained BERT base weights achieves an F1 score of 47.48 on the test set with an accuracy score of 36.87 and obtained 3rd place in the Causal News Corpus 2022 shared task.</pre>
</div>

<script>
function toggleBibtexsaha2022spock2(parameter) {
    var x= document.getElementById('asaha2022spock2');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractsaha2022spock2(parameter) {
    var x= document.getElementById('bsaha2022spock2');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="saha2022spock">Saha, A., Ni, J., Hassanzadeh, O., Gittens, A., Srinivas, K., &amp; Yener, B. (2022). SPOCK at FinCausal 2022: Causal Information Extraction Using Span-Based and Sequence Tagging Models. <i>Proceedings of the 4th Financial Narrative Processing Workshop@ LREC2022</i>, 108–111.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="saha2022spock">Saha, A., Ni, J., Hassanzadeh, O., Gittens, A., Srinivas, K., &amp; Yener, B. (2022). SPOCK at FinCausal 2022: Causal Information Extraction Using Span-Based and Sequence Tagging Models. <i>Proceedings of the 4th Financial Narrative Processing Workshop@ LREC2022</i>, 108–111.</span></div> -->
<!-- <span id="saha2022spock"><b>Saha, A.</b>, Ni, J., Hassanzadeh, O., Gittens, A., Srinivas, K., &amp; Yener, B. (2022). SPOCK at FinCausal 2022: Causal Information Extraction Using Span-Based and Sequence Tagging Models. <i>Proceedings of the 4th Financial Narrative Processing Workshop@ LREC2022</i>, 108–111.</span></div> -->





<a href="/papers/saha2022spock.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>





<button class="btn btn-danger btm-sm" onclick="toggleBibtexsaha2022spock()">BIB</button>



<button class="btn btn-warning btm-sm" onclick="toggleAbstractsaha2022spock()">ABSTRACT</button>



<div id="asaha2022spock" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{saha2022spock,
  abbr = {FinCausal2022},
  title = {SPOCK at FinCausal 2022: Causal Information Extraction Using Span-Based and Sequence Tagging Models},
  author = {Saha, Anik and Ni, Jian and Hassanzadeh, Oktie and Gittens, Alex and Srinivas, Kavitha and Yener, Bulent},
  booktitle = {Proceedings of the 4th Financial Narrative Processing Workshop@ LREC2022},
  pages = {108--111},
  file = {saha2022spock.pdf},
  year = {2022}
}
</pre>
</div>


<div id="bsaha2022spock" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Causal information extraction is an important task in natural language processing, particularly in finance domain. In this work, we develop several information extraction models using pre-trained transformer-based language models for identifying cause and effect text spans from financial documents. We use FinCausal 2021 and 2022 data sets to train span-based and sequence tagging models. Our ensemble of sequence tagging models based on the RoBERTa-Large pre-trained language model achieves an F1 score of 94.70 with Exact Match score of 85.85 and obtains the 1st place in the FinCausal 2022 competition.</pre>
</div>

<script>
function toggleBibtexsaha2022spock(parameter) {
    var x= document.getElementById('asaha2022spock');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractsaha2022spock(parameter) {
    var x= document.getElementById('bsaha2022spock');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="saha2021position">Saha, A., Finegan-Dollak, C., &amp; Verma, A. (2021). Position Masking for Improved Layout-Aware Document Understanding. <i>Document Intelligence Workshop at KDD 2021</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="saha2021position">Saha, A., Finegan-Dollak, C., &amp; Verma, A. (2021). Position Masking for Improved Layout-Aware Document Understanding. <i>Document Intelligence Workshop at KDD 2021</i>.</span></div> -->
<!-- <span id="saha2021position"><b>Saha, A.</b>, Finegan-Dollak, C., &amp; Verma, A. (2021). Position Masking for Improved Layout-Aware Document Understanding. <i>Document Intelligence Workshop at KDD 2021</i>.</span></div> -->





<a href="/papers/saha2021position.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>





<button class="btn btn-danger btm-sm" onclick="toggleBibtexsaha2021position()">BIB</button>



<button class="btn btn-warning btm-sm" onclick="toggleAbstractsaha2021position()">ABSTRACT</button>



<div id="asaha2021position" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{saha2021position,
  abbr = {DI@KDD},
  title = {Position Masking for Improved Layout-Aware Document Understanding},
  author = {Saha, Anik and Finegan-Dollak, Catherine and Verma, Ashish},
  booktitle = {Document Intelligence Workshop at KDD 2021},
  arxiv = {2109.00442},
  pdf = {https://document-intelligence.github.io/DI-2021/files/di-2021_final_21.pdf},
  slides = {https://document-intelligence.github.io/DI-2021/files/di-2021_slides_21.pptx},
  video = {https://youtu.be/2XqrIHexte0},
  file = {saha2021position.pdf},
  year = {2021}
}
</pre>
</div>


<div id="bsaha2021position" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Natural language processing for document scans and PDFs has the potential to enormously improve the efficiency of business processes. Layout-aware word embeddings such as LayoutLM have shown promise for classification of and information extraction from such documents. This paper proposes a new pre-training task called position masking that can improve performance of layout-aware word embeddings that incorporate 2-D position embeddings. We compare models pre-trained with only language masking against models pre-trained with both language masking and position masking, and we find that position masking improves performance by over 5% on a form understanding task.</pre>
</div>

<script>
function toggleBibtexsaha2021position(parameter) {
    var x= document.getElementById('asaha2021position');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractsaha2021position(parameter) {
    var x= document.getElementById('bsaha2021position');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>
</div>

<div class="jumbotron">
  <h3 id="preprints">Preprints</h3>
  <ol class="bibliography" reversed="reversed"><li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="saha2023crossdomain">Saha, A., Hassanzadeh, O., Gittens, A., Ni, J., Srinivas, K., &amp; Yener, B. (2023). A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction. In <i>arXiv preprint arXiv:2308.03891</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="saha2023crossdomain">Saha, A., Hassanzadeh, O., Gittens, A., Ni, J., Srinivas, K., &amp; Yener, B. (2023). A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction. In <i>arXiv preprint arXiv:2308.03891</i>.</span></div> -->
<!-- <span id="saha2023crossdomain"><b>Saha, A.</b>, Hassanzadeh, O., Gittens, A., Ni, J., Srinivas, K., &amp; Yener, B. (2023). A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction. In <i>arXiv preprint arXiv:2308.03891</i>.</span></div> -->





<a href="/papers/saha2023crossdomain.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>





<button class="btn btn-danger btm-sm" onclick="toggleBibtexsaha2023crossdomain()">BIB</button>



<button class="btn btn-warning btm-sm" onclick="toggleAbstractsaha2023crossdomain()">ABSTRACT</button>



<div id="asaha2023crossdomain" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{saha2023crossdomain,
  title = {A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction},
  author = {Saha, Anik and Hassanzadeh, Oktie and Gittens, Alex and Ni, Jian and Srinivas, Kavitha and Yener, Bulent},
  journal = {arXiv preprint arXiv:2308.03891},
  year = {2023},
  eprint = {2308.03891},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  file = {saha2023crossdomain.pdf}
}
</pre>
</div>


<div id="bsaha2023crossdomain" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Causal knowledge extraction is the task of extracting relevant causes and effects from text by detecting the causal relation. Although this task is important for language understanding and knowledge discovery, recent works in this domain have largely focused on binary classification of a text segment as causal or non-causal. In this regard, we perform a thorough analysis of three sequence tagging models for causal knowledge extraction and compare it with a span based approach to causality extraction. Our experiments show that embeddings from pre-trained language models (e.g. BERT) provide a significant performance boost on this task compared to previous state-of-the-art models with complex architectures. We observe that span based models perform better than simple sequence tagging models based on BERT across all 4 data sets from diverse domains with different types of cause-effect phrases.</pre>
</div>

<script>
function toggleBibtexsaha2023crossdomain(parameter) {
    var x= document.getElementById('asaha2023crossdomain');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractsaha2023crossdomain(parameter) {
    var x= document.getElementById('bsaha2023crossdomain');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="saha2023improving">Saha, A., Hassanzadeh, O., Gittens, A., Ni, J., Srinivas, K., &amp; Yener, B. (2023). Improving Neural Ranking Models with Traditional IR Methods. In <i>arXiv preprint arXiv:2308.15027</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="saha2023improving">Saha, A., Hassanzadeh, O., Gittens, A., Ni, J., Srinivas, K., &amp; Yener, B. (2023). Improving Neural Ranking Models with Traditional IR Methods. In <i>arXiv preprint arXiv:2308.15027</i>.</span></div> -->
<!-- <span id="saha2023improving"><b>Saha, A.</b>, Hassanzadeh, O., Gittens, A., Ni, J., Srinivas, K., &amp; Yener, B. (2023). Improving Neural Ranking Models with Traditional IR Methods. In <i>arXiv preprint arXiv:2308.15027</i>.</span></div> -->





<a href="/papers/saha2023improving.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>





<button class="btn btn-danger btm-sm" onclick="toggleBibtexsaha2023improving()">BIB</button>



<button class="btn btn-warning btm-sm" onclick="toggleAbstractsaha2023improving()">ABSTRACT</button>



<div id="asaha2023improving" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{saha2023improving,
  title = {Improving Neural Ranking Models with Traditional IR Methods},
  author = {Saha, Anik and Hassanzadeh, Oktie and Gittens, Alex and Ni, Jian and Srinivas, Kavitha and Yener, Bulent},
  journal = {arXiv preprint arXiv:2308.15027},
  year = {2023},
  eprint = {2308.15027},
  archiveprefix = {arXiv},
  primaryclass = {cs.IR},
  file = {saha2023improving.pdf}
}
</pre>
</div>


<div id="bsaha2023improving" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Neural ranking methods based on large transformer models have recently gained significant attention in the information retrieval community, and have been adopted by major commercial solutions. Nevertheless, they are computationally expensive to create, and require a great deal of labeled data for specialized corpora. In this paper, we explore a low resource alternative which is a bag-of-embedding model for document retrieval and find that it is competitive with large transformer models fine tuned on information retrieval tasks. Our results show that a simple combination of TF-IDF, a traditional keyword matching method, with a shallow embedding model provides a low cost path to compete well with the performance of complex neural ranking models on 3 datasets. Furthermore, adding TF-IDF measures improves the performance of large-scale fine tuned models on these tasks.</pre>
</div>

<script>
function toggleBibtexsaha2023improving(parameter) {
    var x= document.getElementById('asaha2023improving');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractsaha2023improving(parameter) {
    var x= document.getElementById('bsaha2023improving');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="saha2023word">Saha, A., Gittens, A., &amp; Yener, B. (2023). Word Sense Induction with Knowledge Distillation from BERT. In <i>arXiv preprint arXiv:2109.00442</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="saha2023word">Saha, A., Gittens, A., &amp; Yener, B. (2023). Word Sense Induction with Knowledge Distillation from BERT. In <i>arXiv preprint arXiv:2109.00442</i>.</span></div> -->
<!-- <span id="saha2023word"><b>Saha, A.</b>, Gittens, A., &amp; Yener, B. (2023). Word Sense Induction with Knowledge Distillation from BERT. In <i>arXiv preprint arXiv:2109.00442</i>.</span></div> -->





<a href="/papers/saha2023word.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>





<button class="btn btn-danger btm-sm" onclick="toggleBibtexsaha2023word()">BIB</button>



<button class="btn btn-warning btm-sm" onclick="toggleAbstractsaha2023word()">ABSTRACT</button>



<div id="asaha2023word" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{saha2023word,
  title = {Word Sense Induction with Knowledge Distillation from BERT},
  author = {Saha, Anik and Gittens, Alex and Yener, Bulent},
  journal = {arXiv preprint arXiv:2109.00442},
  year = {2023},
  eprint = {2304.10642},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  file = {saha2023word.pdf}
}
</pre>
</div>


<div id="bsaha2023word" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Pre-trained contextual language models are ubiquitously employed for language understanding tasks, but are unsuitable for resource-constrained systems. Noncontextual word embeddings are an efficient alternative in these settings. Such methods typically use one vector to encode multiple different meanings of a word, and incur errors due to polysemy. This paper proposes a two-stage method to distill multiple word senses from a pre-trained language model (BERT) by using attention over the senses of a word in a context and transferring this sense information to fit multi-sense embeddings in a skip-gram-like framework. We demonstrate an effective approach to training the sense disambiguation mechanism in our model with a distribution over word senses extracted from the output layer embeddings of BERT. Experiments on the contextual word similarity and sense induction tasks show that this method is superior to or competitive with state-of-the-art multi-sense embeddings on multiple benchmark data sets, and experiments with an embedding-based topic model (ETM) demonstrates the benefits of using this multi-sense embedding in a downstream application.</pre>
</div>

<script>
function toggleBibtexsaha2023word(parameter) {
    var x= document.getElementById('asaha2023word');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractsaha2023word(parameter) {
    var x= document.getElementById('bsaha2023word');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>
</div>

</div>

      </div>
    </div>

    <br/>
<section id="footer">
<div class="container-footer">
  <div class="panel-footer">
	  <div class="row">
		<div class="col-sm-4">
		    <h5>About</h5>	
            <p>Anik Saha<br/> AI Engineer
</p>
		</div>

		<div class="col-sm-4">
		    <h5>Contact</h5>	
            <p><a href="mailto:aaniksaha@gmail.com" target="_blank"><i class="fa fa-envelope fa-1x"></i> Contact Anik via email</a> <br/> <a href="https://github.com/sbryngelson/academic-website-template"><i class="fa fa-github fa-1x"></i> Use this site as a template!</a>
</p>
		</div>

		<div class="col-sm-4">
		    <h5>Coordinates</h5>	
            <p>Chicago, IL
</p>
		</div>
	  </div>

      <center><p>&copy 2024 Anik Saha </p></center>
	</div>
  </div>
</div>

<script src="/assets/javascript/bootstrap/jquery.min.js"></script>
<script src="/assets/javascript/bootstrap/bootstrap.bundle.min.js"></script>


  </body>

</html>
